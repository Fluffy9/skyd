package main

import (
	"encoding/json"
	"flag"
	"fmt"
	"gitlab.com/NebulousLabs/Sia/encoding"
	"gitlab.com/NebulousLabs/Sia/modules"
	"gitlab.com/NebulousLabs/Sia/modules/gateway"
	"log"
	"os"
	"path/filepath"
	"strings"
	"time"
)

type nodeScanner struct {
	//The node scanner uses a dummy gateway to connect to nodes and
	//		requests peers from nodes across the network using the
	//		ShareNodes RPC.
	gateway *gateway.Gateway

	// Multiple workers are given addresses to scan using workCh.
	// The workers always send a result back to the main goroutine
	// using the resultCh
	workCh   chan workAssignment
	resultCh chan nodeScanResult

	/*
		Count the total number of work assignments sent down workCh
		 and the total number of results received through resultCh.

		 Since every work assignment sent always sends a result back
		 (even in case of failure), the main goroutine can tell if the
		 node scan has finished by checking that:
			- there are no assignments outstanding in workCh
			- there are no unprocessed results in resultCh
			- there are no unassigned addresses in queue
			- all workers are done with their assignements (totalWorkAssignments == totalResults)
	*/
	totalWorkAssignments int
	totalResults         int

	// The seen set keeps track of all the addresses seen by the
	// scanner so far.
	seen map[modules.NetAddress]struct{}
	// The queue holds nodes to be added to workCh.
	queue []modules.NetAddress

	// Connection stats for the current scan.
	stats scannerStats

	// scanLog holds all the results for this scan.
	scanLog *os.File
	// encoder is used to store nodeScanResult structs
	// as JSON objects in the scanLog.
	encoder *json.Encoder

	// The persist manages a persistData object that keeps
	// track of the time of the last successful connection
	// to every single node the NodeScannner has scanned.
	persist *persist
}

// workAssignment tells a worker which node it should scan,
// and the number of times it should send the ShareNodes RPC.
// The ShareNodes RPC is used multiple times because nodes will
// only return 10 random peers, but we want as many as possible.
type workAssignment struct {
	node           modules.NetAddress
	maxRPCAttempts int
}

// nodeScanResult gives the set of nodes received from ShareNodes
// RPCs sent to a specific node. err is nil, an error from connecting,
// or an error from ShareNodes.
type nodeScanResult struct {
	Addr      modules.NetAddress
	Timestamp int64
	Err       error
	nodes     map[modules.NetAddress]struct{}
}

// Counters generated by the node scanner.
type scannerStats struct {
	// Counters for successful/unsuccessful connections.
	SuccessfulConnections int
	FailedConnections     int

	// Counters for common failure types.
	UnacceptableVersionFailures  int
	NetworkIsUnreachableFailures int
	NoRouteToHostFailures        int
	ConnectionRefusedFailures    int
	ConnectionTimedOutFailures   int
	AlreadyConnectedFailures     int
}

const maxSharedNodes = uint64(1000)
const maxRPCs = 10
const maxWorkers = 10
const workChSize = 1000

// pruneAge is the maxiumum allowed time in seconds since the last successful connection with a
// node before we remove it from the persisted set. It is 1 month in seconds.
// 60 seconds/minute * 60 minutes/hour * 24 hours/day * 30 days/month
const pruneAge = 60 * 60 * 24 * 30

func main() {
	dirPtr := flag.String("dir", "", "Directory where the node scanner will store its results")
	flag.Parse()

	ns := newNodeScanner(*dirPtr)

	// Start all the workers.
	for i := 0; i < maxWorkers; i++ {
		go startWorker(ns.gateway, ns.workCh, ns.resultCh)
	}

	// Print out stats periodically.
	// Persist the node set periodically.
	printTicker := time.NewTicker(10 * time.Second)
	persistTicker := time.NewTicker(1 * time.Minute)

	numRPCAttempts := 5
	for {
		select {
		case <-printTicker.C:
			fmt.Printf(ns.getStatsStr())

		case <-persistTicker.C:
			log.Println("Persisting nodes: ", len(ns.persist.data.NodeStats))
			ns.persist.persistData()

		case res := <-ns.resultCh:
			ns.totalResults++

			// Update persisted set with result.
			ns.persist.updateNodeStats(res)

			// Add any new nodes from this set of results.
			for node := range res.nodes {
				if _, alreadySeen := ns.seen[node]; !alreadySeen {
					ns.seen[node] = struct{}{}
					ns.queue = append(ns.queue, node)
				}
			}

			// Log the result and any errors.
			ns.logWorkerResult(res)
		}

		// Fill up workCh with nodes from queue.
		var node modules.NetAddress
		for i := len(ns.workCh); i < cap(ns.workCh); i++ {
			if len(ns.queue) == 0 {
				break
			}
			node, ns.queue = ns.queue[len(ns.queue)-1], ns.queue[:len(ns.queue)-1]
			ns.totalWorkAssignments++
			ns.workCh <- workAssignment{
				node:           node,
				maxRPCAttempts: numRPCAttempts,
			}
		}

		// Check ending condition.
		if (len(ns.workCh) == 0) && (len(ns.resultCh) == 0) && (len(ns.queue) == 0) && (ns.totalWorkAssignments == ns.totalResults) {
			fmt.Printf(ns.getStatsStr())

			// Append stats to stats file.
			ns.encoder.Encode(ns.stats)
			ns.scanLog.Close()

			// Save the persistData.
			ns.persist.persistData()
			return
		}
	}
}

// setupNodeScanner creates a dummy gateway at localhost and initializes all the
// data structures used for scanning the network. It starts by connecting to the
// Sia node at addr. It then creates a queue of node addresses using the set of
// bootstrap nodes and also by asking the initial node for peers using the
// ShareNodes RPC.
func newNodeScanner(scannerDirPrefix string) (ns *nodeScanner) {
	ns = new(nodeScanner)
	ns.stats = scannerStats{}

	// Setup the node scanner's directories.
	scannerDirPath := filepath.Join(scannerDirPrefix, "SiaNodeScanner")
	scannerGatewayDirPath := filepath.Join(scannerDirPath, "gateway")
	if _, err := os.Stat(scannerDirPath); os.IsNotExist(err) {
		err := os.Mkdir(scannerDirPath, 0777)
		if err != nil {
			log.Fatal("Error creating scan directory: ", err)
		}
	}
	if _, err := os.Stat(scannerGatewayDirPath); os.IsNotExist(err) {
		err := os.Mkdir(scannerGatewayDirPath, 0777)
		if err != nil {
			log.Fatal("Error creating scanner gateway directory: ", err)
		}
	}
	log.Printf("Logging data in:  %s\n", scannerDirPath)

	// Create the file for this scan.
	startTime := time.Now().Format("01-02:15:04")
	scanLogName := scannerDirPath + "/scan-" + startTime + ".json"
	scanLog, err := os.Create(scanLogName)
	if err != nil {
		log.Fatal("Error creating scan file: ", err)
	}
	ns.scanLog = scanLog
	ns.encoder = json.NewEncoder(ns.scanLog)

	// Create dummy gateway at localhost.
	g, err := gateway.New("localhost:0", true, scannerGatewayDirPath)
	if err != nil {
		log.Fatal("Error making new gateway: ", err)
	}
	log.Println("Set up gateway at address: ", g.Address())
	ns.gateway = g

	persistFileName := scannerDirPath + "/persisted-node-set.json"
	ns.persist, err = newPersist(persistFileName)
	if err != nil {
		log.Fatal("Error creating persist: ", err)
	}

	// If the persisted set is empty, start with bootstrap nodes in queue.
	// Otherwise start off with the persisted node set in the queue.
	if len(ns.persist.data.NodeStats) == 0 {
		log.Println("Starting crawl with bootrstrap peers")
		ns.queue = make([]modules.NetAddress, len(modules.BootstrapPeers))
		copy(ns.queue, modules.BootstrapPeers)
	} else {
		ns.queue = make([]modules.NetAddress, 0, len(ns.persist.data.NodeStats))
		prunedPersistedData := persistData{
			StartTime: ns.persist.data.StartTime,
			NodeStats: make(map[modules.NetAddress]nodeStats),
		}

		now := time.Now().Unix()
		for node, nodeStats := range ns.persist.data.NodeStats {
			// Prune peers we haven't connected to in more than pruneAge
			// by not adding them to the new set.
			if now-nodeStats.LastSuccessfulConnectionTime < pruneAge {
				prunedPersistedData.NodeStats[node] = nodeStats
				ns.queue = append(ns.queue, node)
			}
		}
		ns.persist.data = prunedPersistedData
		log.Printf("Starting crawl with %d persisted peers\n", len(ns.persist.data.NodeStats))
	}

	// Mark all starting nodes as seen.
	ns.seen = make(map[modules.NetAddress]struct{})
	for _, n := range ns.queue {
		ns.seen[n] = struct{}{}
	}

	// Setup worker channels and send initial queue items down.
	ns.workCh = make(chan workAssignment, workChSize)
	ns.resultCh = make(chan nodeScanResult, workChSize)

	var i int
	for ; i < len(ns.queue) && i < cap(ns.workCh); i++ {
		ns.totalWorkAssignments++
		ns.workCh <- workAssignment{
			node:           ns.queue[i],
			maxRPCAttempts: maxRPCs,
		}
	}
	ns.queue = ns.queue[:i]

	log.Printf("Starting with %d nodes in workCh.\n", len(ns.workCh))
	return
}

// logWorkerResult collects the address, timestamp, and error returned
// from the scan of a single node and writes it to the scanLog as a JSON object.
// It also updates internal node scanner counters using the error returned.
func (ns *nodeScanner) logWorkerResult(res nodeScanResult) {
	err := ns.encoder.Encode(res)
	if err != nil {
		log.Println("Error writing nodeScanResult to file! - ", err)
	}

	if res.Err == nil {
		ns.stats.SuccessfulConnections++
		return
	}
	ns.stats.FailedConnections++

	if strings.Contains(res.Err.Error(), "unacceptable version") {
		ns.stats.UnacceptableVersionFailures++
	} else if strings.Contains(res.Err.Error(), "unreachable") {
		ns.stats.NetworkIsUnreachableFailures++
	} else if strings.Contains(res.Err.Error(), "no route to host") {
		ns.stats.NoRouteToHostFailures++
	} else if strings.Contains(res.Err.Error(), "connection refused") {
		ns.stats.ConnectionRefusedFailures++
	} else if strings.Contains(res.Err.Error(), "connection timed out") {
		ns.stats.ConnectionTimedOutFailures++
	} else if strings.Contains(res.Err.Error(), "already connected") {
		ns.stats.AlreadyConnectedFailures++
	} else {
		log.Printf("Cannot connect to local node at address %s: %s\n", res.Addr, res.Err)
	}
}

func (ns *nodeScanner) getStatsStr() string {
	s := fmt.Sprintf("Seen: %d,  Queued: %d, In WorkCh: %d, In ResultCh: %d\n", len(ns.seen), len(ns.queue), len(ns.workCh), len(ns.resultCh))
	s += fmt.Sprintf("Number assigned: %d, Number of results: %d\n", ns.totalWorkAssignments, ns.totalResults)
	s += fmt.Sprintf("Successful Connections: %d, Failed: %d\n\t(Unacceptable version: %d, Unreachable: %d, No Route: %d, Refused: %d, Timed Out: %d, Already Connected: %d)\n\n", ns.stats.SuccessfulConnections, ns.stats.FailedConnections, ns.stats.UnacceptableVersionFailures, ns.stats.NetworkIsUnreachableFailures, ns.stats.NoRouteToHostFailures, ns.stats.ConnectionRefusedFailures, ns.stats.ConnectionTimedOutFailures, ns.stats.AlreadyConnectedFailures)
	return s
}

// startWorker starts a worker that continually receives from the workCh,
// connect to the node it has been assigned, and returns all results
// using resultCh.
func startWorker(g *gateway.Gateway, workCh <-chan workAssignment, resultCh chan<- nodeScanResult) {
	for {
		work := <-workCh

		// Try connecting to the node at this address.
		// If the connection fails, return the error message.
		err := g.Connect(work.node)
		if err != nil {
			resultCh <- nodeScanResult{
				Addr:      work.node,
				Timestamp: time.Now().Unix(),
				Err:       err,
				nodes:     nil,
			}
			continue
		}

		// Try 1 or more ShareNodes RPCs with this node. Return any errors.
		resultCh <- sendShareNodesRequests(g, work)
		g.Disconnect(work.node)
	}
}

const timeBetweenRequests = 50 * time.Millisecond

// Send ShareNodesRequest(s) to a node and return the set of nodes received.
func sendShareNodesRequests(g *gateway.Gateway, work workAssignment) nodeScanResult {
	result := nodeScanResult{
		Addr:      work.node,
		Err:       nil,
		Timestamp: time.Now().Unix(),
		nodes:     make(map[modules.NetAddress]struct{}),
	}

	// The ShareNodes RPC gives at most 10 random peers from the node, so
	// we repeatedly call ShareNodes in an attempt to get more peers quickly.
	for i := 0; i < work.maxRPCAttempts; i++ {
		var newNodes []modules.NetAddress
		result.Err = g.RPC(work.node, "ShareNodes", func(conn modules.PeerConn) error {
			return encoding.ReadObject(conn, &newNodes, maxSharedNodes*modules.MaxEncodedNetAddressLength)
		})
		if result.Err != nil {
			return result
		}
		for _, n := range newNodes {
			result.nodes[n] = struct{}{}
		}

		// Avoid spamming nodes by adding time between RPCs.
		time.Sleep(timeBetweenRequests)
	}

	return result
}
